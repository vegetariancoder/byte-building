{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Process Orders Data\n",
    "1. Ingest the data into the data lakehouse - stg_orders\n",
    "2. Perform data quality checks and transform the data as required - stg_orders_clean\n",
    "3. Apply changes to the Addresses data (SCD Type 2) - raw_orders"
   ],
   "id": "8b06ead021bb1730"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import col, current_timestamp, current_date, explode"
   ],
   "id": "f98edce492f9cd98"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 1. Ingest the data into the data lakehouse - stg_orders",
   "id": "da3a67654a017dc9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "@dlt.table(\n",
    "    name=\"stg_orders\",  # you can assign a schema name here as well: <schema_name>.bronze_orders\n",
    "    comment=\"The orders data ingested from the order's data lakehouse.\",\n",
    "    table_properties={\n",
    "        \"quality\": \"staging\",\n",
    "        \"delta.autoOptimize.optimizeWrite\": \"true\"\n",
    "    }\n",
    ")\n",
    "def stg_orders():\n",
    "    df_orders = spark \\\n",
    "        .readStream \\\n",
    "        .format(\"cloudFiles\") \\\n",
    "        .option(\"cloudFiles.format\", \"json\") \\\n",
    "        .option(\"cloudFiles.inferSchema\", \"true\") \\\n",
    "        .option(\"cloudFiles.inferColumnTypes\", \"true\") \\\n",
    "        .option(\"cloudFiles.schemaLocation\", \"/Volumes/circuitbox/landing/operational_data/schema/orders/\") \\\n",
    "        .load(\"/Volumes/circuitbox/landing/operational_data/orders/\")\n",
    "\n",
    "    df_orders = df_orders \\\n",
    "        .withColumn(\"input_file_path\", col(\"_metadata.file_path\")) \\\n",
    "        .withColumn(\"ingest_timestamp\", current_timestamp()) \\\n",
    "        .withColumn(\"load_date\", current_date())\n",
    "\n",
    "    return df_orders"
   ],
   "id": "14239360f163e692"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 2. Perform data quality checks and transform the data as required - stg_orders_clean",
   "id": "f6ede41cd0ee06ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "@dlt.table(\n",
    "    name=\"stg_orders_clean\",\n",
    "    comment=\"Cleaned orders data\",\n",
    "    table_properties={'quality': 'staging'}\n",
    ")\n",
    "@dlt.expect_or_fail(\"valid_customer_id\", \"customer_id IS NOT NULL\")\n",
    "@dlt.expect_or_drop(\"valid_order_id\", \"order_id IS NOT NULL\")\n",
    "@dlt.expect(\"valid_order_status\", \"order_status IN ('Pending', 'Shipped', 'Cancelled', 'Completed')\")\n",
    "def stg_orders_clean():\n",
    "    df_stg_orders_clean = spark.readStream.table(\"LIVE.stg_orders\") \\\n",
    "        .select(\n",
    "        \"order_id\",\n",
    "        \"customer_id\",\n",
    "        col(\"order_timestamp\").cast(\"date\"),\n",
    "        \"payment_method\",\n",
    "        \"items\",\n",
    "        \"order_status\"\n",
    "    ) \\\n",
    "        .withColumn(\"ingest_timestamp\", current_timestamp()) \\\n",
    "        .withColumn(\"load_date\", current_date())\n",
    "    return df_stg_orders_clean"
   ],
   "id": "50880a2c586e0a0e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 3. Apply changes to the Addresses data (SCD Type 2) - raw_orders",
   "id": "69ae47f907569bde"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "@dlt.table(\n",
    "    name=\"raw_orders\",\n",
    "    comment=\"Flattened streaming orders with exploded items\",\n",
    "    table_properties={'quality': 'raw'}\n",
    ")\n",
    "def silver_orders():\n",
    "\n",
    "    raw_orders_clean = dlt.read_stream(\"stg_orders_clean\")\n",
    "\n",
    "    raw_orders_clean = raw_orders_clean\\\n",
    "    .withColumn(\"item\", explode(col(\"items\")))\\\n",
    "    .select(\n",
    "        \"order_id\",\n",
    "        \"customer_id\",\n",
    "        \"order_timestamp\",\n",
    "        \"payment_method\",\n",
    "        \"order_status\",\n",
    "        col(\"item.item_id\").alias(\"item_id\"),\n",
    "        col(\"item.name\").alias(\"item_name\"),\n",
    "        col(\"item.price\").alias(\"item_price\"),\n",
    "        col(\"item.quantity\").alias(\"item_quantity\"),\n",
    "        col(\"item.category\").alias(\"item_category\")\n",
    "    )\\\n",
    "    .withColumn(\"ingest_timestamp\", current_timestamp()) \\\n",
    "    .withColumn(\"load_date\", current_date())\n",
    "\n",
    "    return raw_orders_clean\n",
    "\n",
    "\n"
   ],
   "id": "142bd31387f59eec"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
